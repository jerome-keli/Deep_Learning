{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerome-keli/Deep_Learning/blob/main/Audiobook_Customer_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predicting Customer Re-Purchases in an Audiobook App**\n",
        "\n",
        "This project explores real customer data from an audiobook platform — focusing solely on audio versions of books. Each entry represents a customer who has made at least one purchase. The aim is to understand patterns in their activity and use those insights to predict whether they are likely to purchase again within the next six months.\n",
        "\n",
        "The motivation is straightforward: customers with a low probability of returning may not be worth targeting with advertising, while those more likely to buy again present opportunities for efficient marketing and higher returns. Beyond prediction, the model also highlights which factors most strongly influence customer retention, offering valuable guidance for growth strategies.\n",
        "\n",
        "The dataset is provided as a .csv file and includes metrics such as:\n",
        "\n",
        "\n",
        "*   Book length – both average and total minutes of all purchases\n",
        "*   Price paid – average and total amounts spent\n",
        "*   Review data – whether a review was given and the score out of 10\n",
        "* Engagement – total minutes listened, completion rate (0–1)\n",
        "* Support requests – number of help interactions\n",
        "* Last visit vs. last purchase – time in days since last transaction\n",
        "\n",
        "Customer ID is included but serves only as an identifier, not a predictive factor.\n",
        "\n",
        "For modelling, the past two years of activity form the input data, while the target is a binary variable indicating whether the customer made a purchase in the six months following that period. The problem is framed as a binary classification task:\n",
        "* 0 – customer did not buy again\n",
        "* 1 – customer made another purchase\n",
        "\n",
        "By predicting customer re-purchase behavior, the work demonstrates how machine learning can support targeted marketing, reduce wasted spend, and reveal the key drivers of loyalty in the audiobook market."
      ],
      "metadata": {
        "id": "C0CHNTibD7QS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKujq66CdH6n"
      },
      "source": [
        "### Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IblcjnpcVGXS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "\n",
        "raw_data = np.loadtxt('/content/Audiobooks_data.csv',delimiter=',')\n",
        "unscaled_inputs_all = raw_data[:,1:-1]\n",
        "targets_all = raw_data[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOBXlDaogruF"
      },
      "source": [
        "Balancing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9yrAwBeogsRo"
      },
      "outputs": [],
      "source": [
        "num_one_targets = int(np.sum(targets_all))\n",
        "zero_targets_counter = 0\n",
        "indices_to_remove = []\n",
        "for i in range(targets_all.shape[0]):\n",
        "    if targets_all[i] == 0:\n",
        "        zero_targets_counter += 1\n",
        "        if zero_targets_counter > num_one_targets:\n",
        "          indices_to_remove.append(i)\n",
        "\n",
        "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
        "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPn6EJmTjq1h"
      },
      "source": [
        "Scaling Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TpEpQEHejpjz"
      },
      "outputs": [],
      "source": [
        "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDYlZwIalyQT"
      },
      "source": [
        "Shuffling Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xT43DCFgly5o"
      },
      "outputs": [],
      "source": [
        "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
        "shuffled_targets = targets_equal_priors[shuffled_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEiOgZZ7umbJ"
      },
      "source": [
        "Splitting it into Train/Validation/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "khEliKKpRmxz"
      },
      "outputs": [],
      "source": [
        "samples_count = shuffled_inputs.shape[0]\n",
        "\n",
        "train_samples_count = int(0.8*samples_count)\n",
        "validation_samples_count = int(0.1*samples_count)\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "train_inputs = shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "\n",
        "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
        "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
        "\n",
        "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
        "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LolGkhxwWVDq"
      },
      "source": [
        "Data for Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wqVKhN1ZWh8I"
      },
      "outputs": [],
      "source": [
        "np.savez('Audiobooks_data_train',inputs=train_inputs,targets=train_targets)\n",
        "np.savez('Audiobooks_data_validation',inputs=validation_inputs,targets=validation_targets)\n",
        "np.savez('Audiobooks_data_test',inputs=test_inputs,targets=test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qWqjzhl3ZoBK"
      },
      "outputs": [],
      "source": [
        "npz = np.load('/content/Audiobooks_data_train.npz')\n",
        "train_inputs, train_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
        "\n",
        "npz = np.load('/content/Audiobooks_data_validation.npz')\n",
        "validation_inputs, validation_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
        "\n",
        "npz = np.load('/content/Audiobooks_data_test.npz')\n",
        "test_inputs, test_targets = npz['inputs'].astype(float), npz['targets'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ35oB5_bJY9"
      },
      "source": [
        "### Model\n",
        "Outline, optimizers, loss, early stopping and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFlbHYT1a_-Y",
        "outputId": "03fbd11a-d0ac-4f3c-e9d4-07fd7bdb802c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 - 1s - 42ms/step - accuracy: 0.7002 - loss: 0.5771 - val_accuracy: 0.7338 - val_loss: 0.5103\n",
            "Epoch 2/100\n",
            "36/36 - 0s - 4ms/step - accuracy: 0.7675 - loss: 0.4643 - val_accuracy: 0.7673 - val_loss: 0.4377\n",
            "Epoch 3/100\n",
            "36/36 - 0s - 8ms/step - accuracy: 0.7846 - loss: 0.4140 - val_accuracy: 0.7919 - val_loss: 0.3949\n",
            "Epoch 4/100\n",
            "36/36 - 0s - 8ms/step - accuracy: 0.7893 - loss: 0.3907 - val_accuracy: 0.8166 - val_loss: 0.3836\n",
            "Epoch 5/100\n",
            "36/36 - 0s - 5ms/step - accuracy: 0.8008 - loss: 0.3789 - val_accuracy: 0.8031 - val_loss: 0.3713\n",
            "Epoch 6/100\n",
            "36/36 - 0s - 8ms/step - accuracy: 0.8027 - loss: 0.3675 - val_accuracy: 0.7919 - val_loss: 0.3696\n",
            "Epoch 7/100\n",
            "36/36 - 0s - 4ms/step - accuracy: 0.8011 - loss: 0.3665 - val_accuracy: 0.8166 - val_loss: 0.3518\n",
            "Epoch 8/100\n",
            "36/36 - 0s - 8ms/step - accuracy: 0.8111 - loss: 0.3565 - val_accuracy: 0.8210 - val_loss: 0.3495\n",
            "Epoch 9/100\n",
            "36/36 - 0s - 9ms/step - accuracy: 0.8061 - loss: 0.3528 - val_accuracy: 0.8098 - val_loss: 0.3495\n",
            "Epoch 10/100\n",
            "36/36 - 0s - 7ms/step - accuracy: 0.8120 - loss: 0.3503 - val_accuracy: 0.8054 - val_loss: 0.3443\n",
            "Epoch 11/100\n",
            "36/36 - 0s - 8ms/step - accuracy: 0.8134 - loss: 0.3446 - val_accuracy: 0.7964 - val_loss: 0.3455\n",
            "Epoch 12/100\n",
            "36/36 - 0s - 4ms/step - accuracy: 0.8125 - loss: 0.3421 - val_accuracy: 0.8166 - val_loss: 0.3406\n",
            "Epoch 13/100\n",
            "36/36 - 0s - 9ms/step - accuracy: 0.8156 - loss: 0.3405 - val_accuracy: 0.8076 - val_loss: 0.3451\n",
            "Epoch 14/100\n",
            "36/36 - 0s - 8ms/step - accuracy: 0.8206 - loss: 0.3368 - val_accuracy: 0.8121 - val_loss: 0.3417\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c46c842a990>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "input_size = 10 #there are 10 predictors\n",
        "output_size = 2 #there are 2 classes\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                             tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "max_epochs = 100\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "model.fit(train_inputs,\n",
        "          train_targets,\n",
        "          batch_size = batch_size,\n",
        "          epochs = max_epochs,\n",
        "          callbacks = [early_stopping],\n",
        "          validation_data = (validation_inputs, validation_targets),\n",
        "          verbose = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I0HXmWbpyUJ"
      },
      "source": [
        "Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBf2-1lujQ4N",
        "outputId": "fb8511f4-3253-40f1-ae1c-a78a3ddb000e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8459 - loss: 0.3004 \n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3Vt4QnP7q4JwwBpUfTBQ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}